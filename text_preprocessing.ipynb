{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f3fb96-a180-4607-a0e8-7301e45aaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d25c42-e085-44c7-8f2a-fceb2889075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (if needed)\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('stopwords')  # For stopword removal\n",
    "nltk.download('punkt_tab') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac84e70f-342f-482b-9a99-c3ef3712a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"OMG!!  I cant believed u getting the job!!! thats gr8! ðŸŽ‰ I'm ðŸ˜± so happi for u!  btw, I still remembr when we last met, u were so ðŸ˜Š\n",
    "nervous abt the interview..lol . Hope u come soon so we can hang ðŸ˜‚ out!  Don't forget to call me 2moro!! ðŸŽ‰\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f9637-c2c9-4a43-8643-4fd644a737d3",
   "metadata": {},
   "source": [
    "#### Tokenization:\n",
    "the process of splitting a piece of text into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b680fbd6-4874-43fa-87ce-694230c28fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', 'I', \"'m\", 'ðŸ˜±', 'so', 'happi', 'for', 'u', '!', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'ðŸ˜Š', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'ðŸ˜‚', 'out', '!', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸŽ‰']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Output the tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f4002-ccae-45ad-aaa5-8a45a193d223",
   "metadata": {},
   "source": [
    "### Stop Words Removal\n",
    "helps focus on the important words in reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bc15f6-adb4-4a53-8c9a-d43b51765532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', 'cant', 'believed', 'u', 'getting', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', \"'m\", 'ðŸ˜±', 'happi', 'u', '!', 'btw', ',', 'still', 'remembr', 'last', 'met', ',', 'u', 'ðŸ˜Š', 'nervous', 'abt', 'interview', '..', 'lol', '.', 'Hope', 'u', 'come', 'soon', 'hang', 'ðŸ˜‚', '!', \"n't\", 'forget', 'call', '2moro', '!', '!', 'ðŸŽ‰']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68805e33-183f-4940-9d00-df75bf455675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', '!', '!', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', 'I', \"'m\", 'ðŸ˜±', 'so', 'happi', 'for', 'u', '!', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'ðŸ˜Š', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'ðŸ˜‚', 'out', '!', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸŽ‰']\n"
     ]
    }
   ],
   "source": [
    "# remove a specific word\n",
    "import string\n",
    "\n",
    "#word_to_remove = 'OMG'  # Word to remove\n",
    "words_to_remove = ['omg!!', 'gr8', 'lol']\n",
    "\n",
    "#filtered_tokens2 = [word for word in tokens if word.lower() != word_to_remove]\n",
    "filtered_tokens2 = []\n",
    "\n",
    "# filtered_tokens2 = [word for word in tokens if word.lower() != word_to_remove]\n",
    "for word in tokens:\n",
    "    if word.lower() != ['omg', 'gr8', 'lol']:\n",
    "        filtered_tokens2.append(word)\n",
    "print(filtered_tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39099098-bd56-402b-a202-9f5567191dc9",
   "metadata": {},
   "source": [
    "#### Stemming:\n",
    "the process of reducing words to their base or root form (e.g., \"running\" to \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec83a2b6-ea0b-48a2-9c37-89753455955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: ['OMG', '!', '!', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', 'I', \"'m\", 'ðŸ˜±', 'so', 'happi', 'for', 'u', '!', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'ðŸ˜Š', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'ðŸ˜‚', 'out', '!', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸŽ‰']\n",
      "Stemmed Text: ['omg', '!', '!', 'i', 'cant', 'believ', 'u', 'get', 'the', 'job', '!', '!', '!', 'that', 'gr8', '!', 'ðŸŽ‰', 'i', \"'m\", 'ðŸ˜±', 'so', 'happi', 'for', 'u', '!', 'btw', ',', 'i', 'still', 'remembr', 'when', 'we', 'last', 'met', ',', 'u', 'were', 'so', 'ðŸ˜Š', 'nervou', 'abt', 'the', 'interview', '..', 'lol', '.', 'hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'ðŸ˜‚', 'out', '!', 'do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸŽ‰']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"Original Text:\", tokens)\n",
    "\n",
    "print(\"Stemmed Text:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d718c225-28e6-436c-9336-d895b9de8387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'children', 'were', 'run', 'quickli', ',', 'they', 'will', 'be', 'play', 'until', 'even', '.']\n"
     ]
    }
   ],
   "source": [
    "def stemmedtext(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens] \n",
    "    \n",
    "text2 = \"The children were running quickly, they will be playing until evening.\"\n",
    "\n",
    "stemmedwords = stemmedtext(text2)\n",
    "print(stemmedwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc4e30-44d8-4936-afb6-5026a8d5a972",
   "metadata": {},
   "source": [
    "#### Lemmatization:\n",
    "similar to stemming, but it reduces words to their dictionary form. For example, \"better\" becomes \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3efdece1-b962-48ff-8522-5de5af7ad0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG!!  I cant believed u getting the job!!! thats gr8! ðŸŽ‰ I'm ðŸ˜± so happi for u!  btw, I still remembr when we last met, u were so ðŸ˜Š\n",
      "nervous abt the interview..lol . Hope u come soon so we can hang ðŸ˜‚ out!  Don't forget to call me 2moro!! ðŸŽ‰\n",
      "['OMG', '!', '!', 'I', 'cant', 'believe', 'u', 'get', 'the', 'job', '!', '!', '!', 'thats', 'gr8', '!', 'ðŸŽ‰', 'I', \"'m\", 'ðŸ˜±', 'so', 'happi', 'for', 'u', '!', 'btw', ',', 'I', 'still', 'remembr', 'when', 'we', 'last', 'meet', ',', 'u', 'be', 'so', 'ðŸ˜Š', 'nervous', 'abt', 'the', 'interview', '..', 'lol', '.', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'ðŸ˜‚', 'out', '!', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', '!', '!', 'ðŸŽ‰']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatizedtext(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
    "\n",
    "text2 = \"The children were running quickly, they will be playing until evening.\"\n",
    "lemmatizedwords = lemmatizedtext(text)\n",
    "print(text)\n",
    "print(lemmatizedwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80878e82-0251-436a-b9ae-8a8a076d2bff",
   "metadata": {},
   "source": [
    "#### Punctuation removal: marks such as commas, periods, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a6cee68-afcc-4010-bd15-ef5b7ec0d69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OMG', 'I', 'cant', 'believed', 'u', 'getting', 'the', 'job', 'thats', 'gr8', 'ðŸŽ‰', 'I', \"'m\", 'ðŸ˜±', 'so', 'happi', 'for', 'u', 'btw', 'I', 'still', 'remembr', 'when', 'we', 'last', 'met', 'u', 'were', 'so', 'ðŸ˜Š', 'nervous', 'abt', 'the', 'interview', '..', 'lol', 'Hope', 'u', 'come', 'soon', 'so', 'we', 'can', 'hang', 'ðŸ˜‚', 'out', 'Do', \"n't\", 'forget', 'to', 'call', 'me', '2moro', 'ðŸŽ‰']\n"
     ]
    }
   ],
   "source": [
    "import string  # Import string module\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove punctuation\n",
    "cleaned_tokens = [word for word in tokens if word not in string.punctuation]\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b68de3-f984-4062-9905-40bd3e1e9d8c",
   "metadata": {},
   "source": [
    "#### Normalizing text\n",
    "means converting text into a consistent format, such as converting all letters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a71ceab3-1617-42bb-b5c5-22587e089587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg!!  i cant believed u getting the job!!! thats gr8! ðŸŽ‰ i'm ðŸ˜± so happi for u!  btw, i still remembr when we last met, u were so ðŸ˜Š\n",
      "nervous abt the interview..lol . hope u come soon so we can hang ðŸ˜‚ out!  don't forget to call me 2moro!! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "# Convert all text to lowercase\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbd056-cd48-452e-9257-23e15358b68b",
   "metadata": {},
   "source": [
    "#### Removing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be9f8a8c-0dba-4746-af93-77d6d509c5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omg!!  i cant believed u getting the job!!! thats gr! ðŸŽ‰ i'm ðŸ˜± so happi for u!  btw, i still remembr when we last met, u were so ðŸ˜Š\n",
      "nervous abt the interview..lol . hope u come soon so we can hang ðŸ˜‚ out!  don't forget to call me moro!! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "import re #regular expressions\n",
    "\n",
    "# Remove numbers\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4d399-3b11-48ce-9b2c-124301b26f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
