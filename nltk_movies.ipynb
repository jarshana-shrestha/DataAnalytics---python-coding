{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56216993-5a9b-483a-905a-daecbe0b6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d7be22-252c-4fc4-8c45-ffeeaca0aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary datasets\n",
    "nltk.download('movie_reviews')  # Movie review dataset\n",
    "nltk.download('stopwords')      # Stopwords for preprocessing\n",
    "nltk.download('punkt')  # Tokenizer for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23cec6b-70ac-4ba7-9ad2-b75b76d87264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Extract movie reviews and their associated labels\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec4dde-8247-4d13-8fb2-5e0649eb0102",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41461756-93c2-471a-a921-f60a337c440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    }
   ],
   "source": [
    "# Example of tokenizing one document for clarity\n",
    "sample_document = documents[0][0]  # Get the first document (words)\n",
    "print(f\"Original Document: {sample_document[:20]}\")  # Print the first 20 words for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820fe486-a437-46e9-b354-9547179f524f",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10f40fd-4f49-4e02-8539-7e67628144e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Words: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
     ]
    }
   ],
   "source": [
    "# Convert all words to lowercase\n",
    "lowercased_words = [word.lower() for word in sample_document]\n",
    "print(f\"Lowercased Words: {lowercased_words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8cd8d-7fab-4c5c-88d7-b2622b2fa9f7",
   "metadata": {},
   "source": [
    "#### Removing Non-Alphabetic Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4d8f10-e630-45ce-ab80-3fa0522232c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabetic Words: ['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Remove words that are not purely alphabetic\n",
    "alphabetic_words = [word for word in lowercased_words if \n",
    "word.isalpha()]\n",
    "print(f\"Alphabetic Words: {alphabetic_words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87a58c-3b56-465b-8646-cf48db4cd416",
   "metadata": {},
   "source": [
    "#### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b05217-7937-490b-83a2-57ad2bb61ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words (No Stopwords): ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal']\n"
     ]
    }
   ],
   "source": [
    "# Load stopwords and remove them from the dataset\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in alphabetic_words if word not in stop_words]\n",
    "print(f\"Filtered Words (No Stopwords): {filtered_words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d6001-84bf-43c9-b970-ee521cbd2b23",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff127deb-3496-47a1-8f1a-c76657317af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 76529, 'a': 38106, 'and': 35576, 'of': 34123, 'to': 31937, 'is': 25195, 'in': 21822, 's': 18513, 'it': 16107, 'that': 15924, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of all words in the dataset\n",
    "all_words = nltk.FreqDist(word.lower() for word in movie_reviews.words() if word.isalpha())\n",
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9724319a-1405-4051-9b3d-4d92e0e8fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 2000 most common words as features\n",
    "word_features = list(all_words.keys())[:2000]\n",
    "\n",
    "# feature extractor function\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {word: (word in document_words) for word in word_features}\n",
    "    return features\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_documents = []\n",
    "for (doc, category) in documents:  \n",
    "    filtered_words = [word.lower() for word in doc if word.isalpha() and word.lower() not in stop_words] \n",
    "    features = document_features(filtered_words) \n",
    "    preprocessed_documents.append((features, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f54d8-b996-4a72-823e-750b49399775",
   "metadata": {},
   "source": [
    "#### Training a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b6d5ee5-a268-4c1e-8806-a328db81536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "train_set = preprocessed_documents[:1600]\n",
    "test_set = preprocessed_documents[1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4e78a5-1f2f-4171-82b9-bd57bfc968b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22b5464-e7f8-4ea2-b561-1a49132b5910",
   "metadata": {},
   "source": [
    "#### Testing and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "767470ff-f4ba-4129-bed8-f2a228a51ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.25%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "print(f\"Accuracy: {accuracy(classifier, test_set) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffebaa89-3b2f-4412-ae80-3fd905151040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test review: neg\n"
     ]
    }
   ],
   "source": [
    "# Test with New Data\n",
    "test_review = \"This movie was absolutely great, with great performances and a good story.\"\n",
    "test_tokens = nltk.word_tokenize(test_review)\n",
    "test_words = [word.lower() for word in test_tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "test_features = document_features(test_words)\n",
    "\n",
    "print(f\"Prediction for test review: {classifier.classify(test_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "403cfcae-7d2e-43e0-a832-31d61e525c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Informative Features:\n",
      "Most Informative Features\n",
      "                   chick = True              neg : pos    =      8.6 : 1.0\n",
      "                 frances = True              pos : neg    =      8.3 : 1.0\n",
      "              undercover = True              neg : pos    =      7.8 : 1.0\n",
      "              derivative = True              neg : pos    =      7.0 : 1.0\n",
      "                  inject = True              neg : pos    =      7.0 : 1.0\n",
      "                 justify = True              neg : pos    =      6.2 : 1.0\n",
      "                   banal = True              neg : pos    =      5.8 : 1.0\n",
      "                bothered = True              neg : pos    =      5.8 : 1.0\n",
      "                     ugh = True              neg : pos    =      5.8 : 1.0\n",
      "                   waste = True              neg : pos    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Display the Most Informative Features\n",
    "print(\"\\nMost Informative Features:\")\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cf00d-dcf4-4921-b4a0-e08039a29861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
