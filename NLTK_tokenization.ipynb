{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c2463e-126b-4015-b93a-f7255f8bc64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') # # This downloads the necessary data for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2228175d-6ce5-4bae-a594-7428fb80e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1faa54-1f29-4090-9745-0d7264d5e759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ram love programming!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Ram love programming!\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b10f133-d125-4ace-9d5b-213bbaadeb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'love', 'programming', '!']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0671440-2a64-4146-a25b-3316e61eb498",
   "metadata": {},
   "source": [
    "#### Sentence tokenization splits a text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ee325f-41d1-4d1b-9ac0-850267fbbe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['I love programming.', \"It's fun!\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#text\n",
    "text = \"I love programming. It's fun!\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8cbb4-a5e9-49e9-a399-691f55ee16a2",
   "metadata": {},
   "source": [
    "#### Character tokenization splits text into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930b3c62-40ec-4a3d-8b43-99198de2e2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization: ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "# word\n",
    "word = \"programming\"\n",
    "\n",
    "# Character Tokenization (manual splitting)\n",
    "char_tokens = list(word)\n",
    "\n",
    "# Print the result\n",
    "print(\"Character Tokenization:\", char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663db88e-503f-46f1-af5b-073efc846f77",
   "metadata": {},
   "source": [
    "#### Whitespace tokenization splits the text based on spaces (whitespace characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c46872-1b21-4274-9745-a72d6cfebe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['I', 'love', 'programming.']\n"
     ]
    }
   ],
   "source": [
    "# sentence\n",
    "text = \"I love programming.\"\n",
    "\n",
    "# Whitespace Tokenization (splitting based on space)\n",
    "whitespace_tokens = text.split( )\n",
    "\n",
    "# Print the result\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005b13d-335b-4f08-a670-e14924e1c8c3",
   "metadata": {},
   "source": [
    "#### Punctuation-Aware Tokenization handles punctuation separately from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a217d72-a3d1-4bfc-92c3-56b811f9ac9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-Aware Tokenization: ['Hello', '!', 'How', 'are', 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence with punctuation\n",
    "text = \"Hello! How are you doing?\"\n",
    "\n",
    "# Word Tokenization (with punctuation handling)\n",
    "word_tokens_with_punct = word_tokenize(text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Punctuation-Aware Tokenization:\", word_tokens_with_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ec088b6-0347-4815-ab5a-b275c88cdd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb4f13eb-82b7-4ffd-9afc-4f8a2ece0524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7950713-6837-45d3-9277-dd4276604c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words List: {'he', 'both', 'doing', 'down', 'above', 'from', 'through', 'than', 'themselves', 'up', 'him', 'just', 'a', 'haven', 'ourselves', 'their', 'for', 'very', 'wouldn', 'why', 'below', 've', 'did', 'too', 'is', 'our', 'd', 'there', \"it'd\", 'she', \"they've\", 'and', \"he'd\", 'to', \"hasn't\", 'm', 'until', \"couldn't\", \"mustn't\", 'shouldn', 'its', 'we', \"shouldn't\", \"isn't\", \"doesn't\", 'will', 'they', 'yourself', 'am', 'such', 'each', 'be', 'mustn', 't', 'herself', \"they'd\", 'because', 'me', 'ain', 'are', 'that', 'shan', 'isn', \"should've\", 're', 'most', 'where', 'how', 'between', \"you'd\", 'once', 'under', 'here', 'these', \"we're\", 'who', 'll', \"didn't\", 'an', 'didn', 'again', 'should', 'more', 'over', 'ours', \"we've\", 'own', \"aren't\", 'when', \"shan't\", 'having', 'only', 'in', 'or', \"we'll\", 'weren', 'out', 'you', 'ma', 'as', 'theirs', 'the', \"she'd\", \"needn't\", 'i', \"he's\", 'y', \"we'd\", 'o', 'needn', 'his', 'during', 'all', \"wouldn't\", 'with', \"she'll\", \"haven't\", 'been', 'after', \"it's\", \"they're\", 'which', 'yours', 'now', 'so', 'hasn', 'can', \"weren't\", 'what', \"she's\", 'while', 'does', 'don', \"i've\", 'himself', \"you'll\", \"mightn't\", 'about', 'have', 'before', \"i'll\", 'your', 'any', 'by', \"i'm\", 'do', 'on', \"i'd\", 'if', 'other', \"you're\", 'was', 'her', \"don't\", 'hers', 'myself', 'further', 'of', 'nor', 'my', 'being', 'couldn', 'mightn', \"it'll\", \"won't\", \"wasn't\", 'against', 'few', \"that'll\", 'them', 'itself', 'into', 's', \"you've\", 'doesn', 'hadn', 'no', 'aren', 'it', 'those', 'were', 'this', \"he'll\", 'some', 'has', 'same', 'won', 'had', 'but', 'at', 'whom', 'yourselves', 'wasn', \"they'll\", 'not', 'then', \"hadn't\", 'off'}\n"
     ]
    }
   ],
   "source": [
    "# Example token list\n",
    "tokens = [\"I\", \"love\", \"programming\", \"and\", \"it\", \"is\", \"fun\"]\n",
    "\n",
    "# Get the list of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stop words list (optional)\n",
    "print(\"Stop Words List:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25a1a835-6151-4999-b194-2ed077d0d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Word Removal: ['love', 'programming', 'fun']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(\"Tokens after Stop Word Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e7d75-9dad-4865-8108-0b43023882aa",
   "metadata": {},
   "source": [
    "#### List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "087953ff-730c-4433-bad2-729661e2fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "long_fruits = []\n",
    "\n",
    "for fruit in basket:\n",
    "    if len(fruit) > 5:\n",
    "        long_fruits.append(fruit)\n",
    "\n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5d4966c-fa50-4f06-b5e3-244dd6517504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# With List Comprehension\n",
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "\n",
    "long_fruits = [fruit for fruit in basket if len(fruit) > 5]\n",
    "\n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42d09e-e2dd-4f80-85d3-a3adfeffbf46",
   "metadata": {},
   "source": [
    "#### Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5776d0da-08e3-4dbc-b169-db4c83626eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l', 'o', 'v', 'e'], ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g'], ['f', 'u', 'n']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Removing special characters\n",
    "def remove_special_chars(tokens):\n",
    " return [re.sub(r'[^A-Za-z0-9]+', '', word) for word in tokens]\n",
    "    \n",
    "reviews_cleaned = [remove_special_chars(tokens) for tokens in filtered_tokens]\n",
    "\n",
    "print(reviews_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbfdc2-785d-45c6-9570-09d6accea798",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe43e099-30bf-496e-a667-7505a63810f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer # Stemming algorithm\n",
    "from nltk.tokenize import word_tokenize # For breaking text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35f16f94-1e5d-43ac-be38-b40aa9039832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'am', 'loving', 'the', 'process', 'of', 'learning', 'and', 'understanding', 'NLP', 'concepts', '.']\n"
     ]
    }
   ],
   "source": [
    "# The Porter Stemmer is a popular algorithm for stemming in English.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample Text\n",
    "text = \"I am loving the process of learning and understanding NLP concepts.\"\n",
    "\n",
    "# Tokenize the Text\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4a4dc40-2e13-440a-a9ad-f3f39ce9f288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['i', 'am', 'love', 'the', 'process', 'of', 'learn', 'and', 'understand', 'nlp', 'concept', '.']\n"
     ]
    }
   ],
   "source": [
    "# apply stemming to each word in the list of tokens\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf9e85-841d-4f6a-bb1c-12e6aed521e8",
   "metadata": {},
   "source": [
    "#### WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438837e4-7f3c-4e16-b9aa-07c1328371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Required Libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d12f55f-7786-4ee1-bb98-3efbdb90c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jarshana\n",
      "[nltk_data]     Shrestha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need WordNet and Punkt for tokenization and lemmatization\n",
    "nltk.download('wordnet') # For lexical database\n",
    "nltk.download('omw-1.4') # Optional for extended multilingual support\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7ef90d9-bdd1-4117-a0b8-3adf4841ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'leaves', 'on', 'the', 'tree', 'were', 'falling', '.', 'She', 'was', 'running', 'quickly', 'but', 'got', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Smaple text\n",
    "text = \"The leaves on the tree were falling. She was running quickly but got tired.\"\n",
    "\n",
    "# Break sentence into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c56f96a2-a5af-4c86-afee-e272b153859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['The', 'leaf', 'on', 'the', 'tree', 'were', 'falling', '.', 'She', 'wa', 'running', 'quickly', 'but', 'got', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "# apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e5054-46da-45fd-93cb-2a4b90b44059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
